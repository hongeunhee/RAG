# -*- coding: utf-8 -*-
"""
Created on Wed May 9 15:34:23 2024

@author: Hui, Hong
"""
#Import all the required libraries
import os
import pandas as pd
from datetime import datetime
from langchain_google_genai import GoogleGenerativeAI
from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
import google.generativeai as genai
from IPython.display import display
from IPython.display import Markdown
import textwrap
from dotenv import load_dotenv #save gemini-pro api key

def merge_text_files_to_dataframe(folder_path):
    # 폴더 내 모든 텍스트 파일 경로 수집
    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]
    
    # 모든 텍스트 파일을 하나의 데이터프레임으로 병합
    parsed_data = []
    columns = ['IP', 'NV1', 'NV2', 'DateTime', 'NV3', 'Request1', 'Request2', 'Protocol', 'ResponseCode', 'Size']

    for file_path in file_paths:
        with open(file_path, 'r') as file:
            for line in file:
                parts = line.split(' ')
                parsed_row = dict(zip(columns, parts))
                parsed_data.append(parsed_row)

    # DataFrame으로 변환
    df = pd.DataFrame(parsed_data)
    # 'DateTime' 열을 기준으로 데이터프레임을 정렬
    df['DateTime'] = df['DateTime'].apply(lambda x: x.replace ('[',''))
    df['DateTime'] = pd.to_datetime(df['DateTime'], format="%d/%b/%Y:%H:%M:%S")
    df.sort_values(by='DateTime', inplace=True)
    return df
 
def filter_logs_within_1_minute(sorted_data):
    
    filtered_logs = []
    
    # 데이터를 반복하면서 필터링
    for i in range(len(sorted_data)):
        current_entry = sorted_data[i]
        current_datetime = current_entry['Datetime']
        current_ip = current_entry['IP']
        current_response_code = current_entry['ResponseCode']
        
        # 현재 로그가 '404' 응답 코드를 가지고 있는지 확인
        if current_response_code in ['404']:
            # 다음 로그부터 1초 이내의 로그를 확인
            for j in range(i+1, len(sorted_data)):
                next_entry = sorted_data[j]
                next_datetime = next_entry['DateTime']
                next_ip = next_entry['IP']
                next_response_code = next_entry['ResponseCode']
                
                # 1초 이내의 로그이고 동일한 IP에 대해 '404'와 '200' 응답 코드를 가지고 있는 경우 필터링
                if (next_datetime - current_datetime).total_seconds() <= 1 and next_ip == current_ip and next_response_code in ['404', '200']:
                    filtered_logs.append(current_entry)
                    break
    
    return filtered_logs

def attacked_web_log(filtered_logs):
    # 1초 이내 404 혹은 200에러 1000개 초과 IP 리스트 추출   
    filtered_logs = pd.DataFrame(filtered_logs).sort_values(by='IP')
    ip_counts = pd.DataFrame(filtered_logs['IP'].value_counts())
    atk_ip = ip_counts[ip_counts['count'] >= 1000]
    atk_ip_lst = atk_ip.index.to_list()
    # 해당 IP에 대한 웹로그 데이터
    atk_info = dataframe[dataframe['IP'].isin(atk_ip_lst)]
    # 중요 패턴을 가진 웹로그만 필터링 
    ptn_plt = atk_info[atk_info['Request2'].str.contains("' OR '1'='1|%|passwd|cgi-bin")]
    # 공격에 성공한 웹로그 필터링
    ptn_plt.loc[:, 'ResponseCode'] = ptn_plt.loc[:, 'ResponseCode'].astype(int)
    atk_scs = ptn_plt[ptn_plt['ResponseCode'].apply(lambda x: eval(f"x {'==200'}"))]
    scan_frt_day = atk_scs['DateTime'].iloc[0]
    atk_scs_count = len(atk_scs)
    
    # 스캔 이후 데이터 값에서 신규 IP와 스캔 과정에서 이상 IP에 대한 데이터만 필터링
    scan_lst_day = atk_scs['DateTime'].iloc[-1]
    scan_b4_ip = dataframe[dataframe['DateTime'] <= scan_lst_day]['IP'].unique().tolist()
    for i in atk_ip_lst:
        scan_b4_ip.remove(i)
    afs = dataframe[dataframe['DateTime'] > scan_lst_day]
    new_ip_info = afs.loc[~(afs['IP'].isin(scan_b4_ip))]
    # ResponseCode 200인 소스코드 필터링
    new_ip_info.loc[:, 'ResponseCode'] = new_ip_info.loc[:, 'ResponseCode'].astype(int)
    new_ip_info_200 = new_ip_info[new_ip_info['ResponseCode'].apply(lambda x: eval(f"x {'==200'}"))]
    
    atk_ip_count = len(atk_ip_lst)
    print(f"취약점 스캔에 사용한 IP는 총 {atk_ip_count}개로 다음과 같습니다.: {atk_ip_lst} ")
    print(f"취약점 스캔 시작 시점은 {scan_frt_day}이고 종료 시점은 {scan_lst_day}입니다. 총 {atk_scs_count}번의 취약점을 스캔했습니다.")
    return atk_scs, new_ip_info_200


#Create a helper function that will convert the markdwon into nicely formatted text
def to_markdown(text):
  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))
 
if __name__=="__main__":
    # 사용자로부터 폴더 경로 입력 받기
    folder_path = input("폴더 경로를 입력하세요: ")
    
    dataframe = merge_text_files_to_dataframe(folder_path)
    dic_df = dataframe.to_dict(orient='records')
    
    # 이상 탐지한 웹로그 출력
    filtered_logs = filter_logs_within_1_minute(dic_df)
    for log in filtered_logs:
        print(log)
        
    ### 분석 결과 출력 단계 ###
    # gemini-agent 구성
    user_input = input("gemini-api 키값을 입력하세요.")
    api_key = user_input
    model = genai.GenerativeModel("models/gemini-1.5-pro-latest")
    ### 기본 출력 ###
    atk_scs, new_ip_info_200 = attacked_web_log(filtered_logs)
    ### 취약점 스캔 기간 동안의 페이로드 분석 결과 출력 ###
    print("취약점 스캔 동안의 페이로드 분석 결과는 다음과 같습니다.")
    lst = atk_scs['Request2'].tolist()
    response = model.generate_content(f"{lst}: 해당 페이로드들을 분석해줘")
    print(to_markdown(response.text))

    ### 취약점 스캔 이후 페이로드 분석 결과 출력 ###
    print("다음은 취약점 스캔 이후 신규 IP와 취약점 스캔에 사용한 IP에 대해 ResponseCode=200을 갖는 페이로드 분석 결과입니다.")
    pay_lst = new_ip_info_200['Request2'].tolist()
    response = model.generate_content(f"{pay_lst}: 페이로드를 분석해 줘. 특별한 점이 없으면 없다고 알려줘")
    print(to_markdown(response.text))
