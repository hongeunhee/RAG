# -*- coding: utf-8 -*-
"""
Created on Fri Jun  7 13:57:59 2024

@author: 202202871
"""

import streamlit as st
import os
import pandas as pd
import google.generativeai as genai
from IPython.display import Markdown, display
import textwrap
from ipaddress import ip_address


# Start Streamlit app
st.set_page_config(page_title="Vulnerability Scanner: Analyze your weblog data", page_icon="🦜")
st.title("🦜 Vulnerability Scanner: Analyze your web log data")

folder_path = st.sidebar.text_input(label="Local file path", type="default")
gemini_api_key = st.sidebar.text_input(label="Gemini API Key", type="password",)

# Check user inputs
if not folder_path:
    st.info("Please enter the folder path for the local weblog data in the sidebar.")
    st.stop()

if not gemini_api_key:
    st.info("Please add your Gemini API key to the sidebar to continue.")
    st.stop()
    
# Verify that the path entered is valid
if folder_path:
    if os.path.isdir(folder_path):
        st.write(f"Entered path: {folder_path}")
        st.write("Folder Content:")
        # Output a list of files and directories in a folder
        for item in os.listdir(folder_path):
            st.write(item)
    else:
        st.error("Please enter a valid folder path.")
else:
    st.write("Please enter your folder path.")

def merge_text_files_to_dataframe(folder_path):
    # 폴더 내 모든 텍스트 파일 수집
    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]
    
    # 모든 텍스트 파일을 하나의 데이터프레임으로 병합
    parsed_data = []
    columns = ['IP', 'NV1', 'NV2', 'DateTime', 'NV3', 'Request1', 'Request2', 'Protocol', 'ResponseCode', 'Size']

    for file_path in file_paths:
        with open(file_path, 'r') as file:
            for line in file:
                parts = line.split(' ')
                parsed_row = dict(zip(columns, parts))
                parsed_data.append(parsed_row)

    # DataFrame으로 변환
    df = pd.DataFrame(parsed_data)
    # 'DateTime' 열을 기준으로 데이터프레임을 정렬
    df['DateTime'] = df['DateTime'].apply(lambda x: x.replace ('[',''))
    df['DateTime'] = pd.to_datetime(df['DateTime'], format="%d/%b/%Y:%H:%M:%S")
    df.sort_values(by='DateTime', inplace=True)
    return df
 
def filter_logs_within_1_minute(sorted_data):
    
    filtered_logs = []
    
    # 데이터를 반복하면서 필터링
    for i in range(len(sorted_data)):
        current_entry = sorted_data[i]
        current_datetime = current_entry['DateTime']
        current_ip = current_entry['IP']
        current_response_code = current_entry['ResponseCode']
        
        # 현재 로그가 '404' 응답 코드를 가지고 있는지 확인
        if current_response_code in ['404']:
            # 다음 로그부터 1초 이내의 로그를 확인
            for j in range(i+1, len(sorted_data)):
                next_entry = sorted_data[j]
                next_datetime = next_entry['DateTime']
                next_ip = next_entry['IP']
                next_response_code = next_entry['ResponseCode']
                
                # 1초 이내의 로그이고 동일한 IP에 대해 '404'와 '200' 응답 코드를 가지고 있는 경우 필터링
                if (next_datetime - current_datetime).total_seconds() <= 1 and next_ip == current_ip and next_response_code in ['404', '200']:
                    filtered_logs.append(current_entry)
                    break
    
    return filtered_logs

def attacked_web_log(filtered_logs):
    # 1초 이내 404 혹은 200에러 1000개 초과 IP 리스트 추출   
    filtered_logs = pd.DataFrame(filtered_logs).sort_values(by='IP')
    ip_counts = pd.DataFrame(filtered_logs['IP'].value_counts())
    ip_counts.columns = ['count']
    atk_ip = ip_counts[ip_counts['count'] >= 1000]
    atk_ip_lst = atk_ip.index.to_list()
    # 해당 IP에 대한 웹로그 데이터
    atk_info = dataframe[dataframe['IP'].isin(atk_ip_lst)]
    # 중요 패턴을 가진 웹로그만 필터링 
    ptn_plt = atk_info[atk_info['Request2'].str.contains("' OR '1'='1|%|passwd|cgi-bin")]
    # 공격에 성공한 웹로그 필터링
    ptn_plt.loc[:, 'ResponseCode'] = ptn_plt.loc[:, 'ResponseCode'].astype(int)
    atk_scs = ptn_plt[ptn_plt['ResponseCode'].apply(lambda x: eval(f"x {'==200'}"))]
    scan_frt_day = atk_scs['DateTime'].iloc[0]
    atk_scs_count = len(atk_scs)
    
    # 스캔 이후 데이터 값에서 신규 IP와 스캔 과정에서 이상 IP에 대한 데이터만 필터링
    scan_lst_day = atk_scs['DateTime'].iloc[-1]
    scan_b4_ip = dataframe[dataframe['DateTime'] <= scan_lst_day]['IP'].unique().tolist()
    for i in atk_ip_lst:
        scan_b4_ip.remove(i)
    afs = dataframe[dataframe['DateTime'] > scan_lst_day]
    new_ip_info = afs.loc[~(afs['IP'].isin(scan_b4_ip))]

    # ResponseCode 200인 소스코드 필터링
    new_ip_info.loc[:, 'ResponseCode'] = new_ip_info.loc[:, 'ResponseCode'].astype(int)
    new_ip_info_200 = new_ip_info[new_ip_info['ResponseCode'].apply(lambda x: eval(f"x {'==200'}"))]
   
    # Size가 평균의 10배 이상인 소스코드 필터링
    new_ip_info_200.loc[:, 'Size'] = new_ip_info_200.loc[:, 'Size'].astype(int)
    size_lst = new_ip_info_200['Size'].tolist()
    size_avg = sum(size_lst)/len(size_lst)
    abnormal = new_ip_info_200[new_ip_info_200['Size'] >= size_avg*10]

    atk_ip_count = len(atk_ip_lst)
    st.write(f"There are a total of {atk_ip_count} IPs used to scan vulnerabilities, and the IPs are as follows: {atk_ip_lst}")
    st.write(f"취약점 스캔 시작 시점은 {scan_frt_day}이고 종료 시점은 {scan_lst_day}입니다. 총 {atk_scs_count}번의 취약점을 스캔했습니다.")
    
    return atk_scs, abnormal
    
def get_public_ips(ip_list):
    public_ips = []
    
    for ip in ip_list:
        if not is_private_ip(ip):
            public_ips.append(ip)
    
    return public_ips
 
# 사설 IP 주소 여부를 확인하는 함수
def is_private_ip(ip):
    private_ip_ranges = [
("10.0.0.0", "10.255.255.255"),
("172.16.0.0", "172.31.255.255"),
("192.168.0.0", "192.168.255.255")
    ]
    
    for start, end in private_ip_ranges:
        if ip_address(start) <= ip_address(ip) <= ip_address(end):
            return True
    return False
 
#Create a helper function that will convert the markdwon into nicely formatted text
def to_markdown(text):
  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))
 
if __name__=="__main__":
    
    dataframe = merge_text_files_to_dataframe(folder_path)
    dic_df = dataframe.to_dict(orient='records')
    
    # 이상 탐지한 웹로그 출력
    filtered_logs = filter_logs_within_1_minute(dic_df)
    
    ### 분석 결과 출력 단계 ###
    # gemini-agent 구성
    #api_key = 'AIzaSyAij_2eLy-nzQVRDouKKur-1TfObHYi3E8' 
    genai.configure(api_key = gemini_api_key) #transport='grpc_asyncio' The user can pass a string to choose 'rest' or 'grpc' or 'grpc_asyncio'
    model = genai.GenerativeModel("models/gemini-1.5-pro-latest")
    ### 기본 출력 ###
    atk_scs, abnormal = attacked_web_log(filtered_logs)
    ### 취약점 스캔 기간 동안의 페이로드 분석 결과 출력 ###
    st.write("취약점 스캔 동안의 페이로드 분석 결과는 다음과 같습니다.")
    lst = atk_scs['Request2'].tolist()
    response = model.generate_content(f"{lst}: 해당 페이로드들을 분석해줘")
    st.write(response['text'])
    ### 취약점 스캔 이후 페이로드 분석 결과 출력 ###
    st.write("다음은 취약점 스캔 이후 페이로드 분석 결과입니다.")
    pay_lst = abnormal['Request2'].tolist()
    response = model.generate_content(f"{pay_lst}: 페이로드를 분석해 줘. 특별한 점이 없으면 없다고 알려줘")
    st.write(response['text'])
    st.write("다음은 취약점 스캔 이후 API 보안 취약점에 대한 분석 결과입니다.")
    response = model.generate_content(f"{pay_lst}: OWASP API 보안 TOP10에 해당하는 취약점이 있으면 알려줘. 특별한 점이 없으면 없다고 알려줘")
    st.write(response['text'])
