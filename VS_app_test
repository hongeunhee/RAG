# -*- coding: utf-8 -*-
"""
Created on Fri Jun  7 13:57:59 2024

@author: 202202871
"""

import streamlit as st
import os
import pandas as pd
import google.generativeai as genai
from IPython.display import Markdown, display
import textwrap
from ipaddress import ip_address
import re
from collections import Counter

# Start Streamlit app
st.set_page_config(page_title="Vulnerability Scanner: Analyze your weblog data", page_icon="🔍")
st.title("🔎 Vulnerability Scanner: Analyze your weblog data")

folder_path = st.sidebar.text_input(label="File path", type="default")
gemini_api_key = st.sidebar.text_input(label="Gemini API Key", type="password",)
    
# Check user inputs
if not folder_path:
    st.info("Please enter the folder path for the weblog data in the sidebar.")
    st.stop()

if not gemini_api_key:
    st.info("Please add your Gemini API key to the sidebar to continue.")
    st.stop()
    
# Verify that the path entered is valid
if folder_path:
    if os.path.isdir(folder_path):
        st.write(f"Entered path: {folder_path}")
        st.write("Folder Content:")
        # Output a list of files and directories in a folder
        for item in os.listdir(folder_path):
            st.write(item)
    else:
        st.error("Please enter a valid folder path.")
else:
    st.write("Please enter your folder path.")

# Functions needed to automatically identify delimiters and column names for multiple txt files with different user input methods and convert them into a single panda data frame
def find_best_delimiter(lines):
    delimiters = [r'\s+', ',', ';', '\t']
    best_delimiter = None
    max_parts = 0

    for delim in delimiters:
        total_parts = sum(len(re.split(delim, line.strip())) for line in lines)
        if total_parts > max_parts:
            best_delimiter = delim
            max_parts = total_parts

    return best_delimiter

def auto_split(line, delimiter):
    parts = re.split(delimiter, line.strip())
    return parts

def parse_file(file_path):
    parsed_data = []
    try:
        with open(file_path, 'r') as file:
            lines = file.readlines()
            delimiter = find_best_delimiter(lines)
            for line in lines:
                parts = auto_split(line, delimiter)
                parsed_data.append(parts)
    except FileNotFoundError as e:
        print(f"File not found: {e.filename}")
    except Exception as e:
        print(f"An error occurred: {e}")

    return parsed_data

def parse_folder(folder_path):
    all_parsed_data = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if not os.path.isfile(file_path) or not filename.endswith('.txt'):
            continue
        parsed_data = parse_file(file_path)
        all_parsed_data.extend(parsed_data)
    return all_parsed_data

def infer_column_names(parsed_lines):
    if not parsed_lines:
        return []

    ip_pattern = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')
    date_pattern = re.compile(r'\[?\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2}\]?')
    request_pattern = re.compile(r'\/[^\s]+')
    
    sample_line = parsed_lines[0]
    column_names = []

    for part in sample_line:
        if ip_pattern.match(part):
            column_names.append('ip_address')
        elif date_pattern.match(part):
            column_names.append('timestamp')
        elif request_pattern.match(part):
            column_names.append('request')
        elif part.isdigit() and len(part) == 3:
            column_names.append('status_code')
        elif part.isdigit() and 1 <= len(part) <= 4 or part == '-':
            column_names.append('size')
        else:
            column_names.append('unknown')
        
    counts = Counter(column_names)
    for i, name in enumerate(column_names):
        if counts[name] > 1:
            suffix = counts[name]
            column_names[i] = f'{name}_{suffix}'
            counts[name] -= 1

    return column_names

def parse_folder_to_dataframe(folder_path):
    parsed_data = parse_folder(folder_path)
    if not parsed_data:
        return pd.DataFrame()
    columns = infer_column_names(parsed_data) #자동 식별 어려울 때 열이름 사용자에게 받는 부분 추가해야 함
    df = pd.DataFrame(parsed_data, columns=columns)
    #df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce') #테스트 해봐야 함
    df['timestamp'] = pd.to_datetime(df['timestamp'], format='[%d/%b/%Y:%H:%M:%S', errors='coerce') 
    dataframe = df.sort_values(by='timestamp', ascending=True)
    return dataframe
     
def filter_logs_within_1_minute(sorted_data):
    
    filtered_logs = []
    
    # 데이터를 반복하면서 필터링
    for i in range(len(sorted_data)):
        current_entry = sorted_data[i]
        current_datetime = current_entry['timestamp']
        current_ip = current_entry['ip_address']
        current_response_code = current_entry['status_code']
        
        # 현재 로그가 '404' 응답 코드를 가지고 있는지 확인
        if current_response_code in ['404']:
            # 다음 로그부터 1초 이내의 로그를 확인
            for j in range(i+1, len(sorted_data)):
                next_entry = sorted_data[j]
                next_datetime = next_entry['timestamp']
                next_ip = next_entry['ip_address']
                next_response_code = next_entry['status_code']
                
                # 1초 이내의 로그이고 동일한 IP에 대해 '404'와 '200' 응답 코드를 가지고 있는 경우 필터링
                if (next_datetime - current_datetime).total_seconds() <= 1 and next_ip == current_ip and next_response_code in ['404', '200']:
                    filtered_logs.append(current_entry)
                    break
    
    return filtered_logs

def attacked_web_log(filtered_logs):
    # 1초 이내 404 혹은 200에러 1000개 초과 IP 리스트 추출   
    filtered_logs = pd.DataFrame(filtered_logs).sort_values(by='ip_address')
    ip_counts = pd.DataFrame(filtered_logs['ip_address'].value_counts())
    ip_counts.columns = ['count']
    atk_ip = ip_counts[ip_counts['count'] >= 1000]
    atk_ip_lst = atk_ip.index.to_list()
    # 해당 IP에 대한 웹로그 데이터
    atk_info = dataframe[dataframe['ip_address'].isin(atk_ip_lst)]
    # 중요 패턴을 가진 웹로그만 필터링 
    ptn_plt = atk_info[atk_info['request'].str.contains("' OR '1'='1|%|passwd|cgi-bin")]
    # 공격에 성공한 웹로그 필터링
    ptn_plt.loc[:, 'status_code'] = ptn_plt.loc[:, 'status_code'].astype(int)
    atk_scs = ptn_plt[ptn_plt['status_code'].apply(lambda x: eval(f"x {'==200'}"))]
    scan_frt_day = atk_scs['timestamp'].iloc[0]
    atk_scs_count = len(atk_scs)
    
    # 스캔 이후 데이터 값에서 신규 IP와 스캔 과정에서 이상 IP에 대한 데이터만 필터링
    scan_lst_day = atk_scs['timestamp'].iloc[-1]
    scan_b4_ip = dataframe[dataframe['timestamp'] <= scan_lst_day]['ip_address'].unique().tolist()
    for i in atk_ip_lst:
        scan_b4_ip.remove(i)
    afs = dataframe[dataframe['timestamp'] > scan_lst_day]
    new_ip_info = afs.loc[~(afs['ip_address'].isin(scan_b4_ip))]

    # ResponseCode 200인 소스코드 필터링
    new_ip_info.loc[:, 'status_code'] = new_ip_info.loc[:, 'status_code'].astype(int)
    new_ip_info_200 = new_ip_info[new_ip_info['status_code'].apply(lambda x: eval(f"x {'==200'}"))]
   
    # Size가 평균의 10배 이상인 소스코드 필터링
    new_ip_info_200.loc[:, 'size'] = new_ip_info_200.loc[:, 'size'].astype(int)
    size_lst = new_ip_info_200['size'].tolist()
    size_avg = sum(size_lst)/len(size_lst)
    abnormal = new_ip_info_200[new_ip_info_200['size'] >= size_avg*10]

    atk_ip_count = len(atk_ip_lst)
    st.write(f"There are a total of {atk_ip_count} IPs used to scan vulnerabilities, and the IPs are as follows: {atk_ip_lst}")
    st.write(f"Vulnerability scan start point is {scan_frt_day} and end point is {scan_lst_day}. Hacker have scanned a total of {atk_scs_count} vulnerabilities.")
    
    return atk_scs, abnormal
    
def get_public_ips(ip_list):
    public_ips = []
    
    for ip in ip_list:
        if not is_private_ip(ip):
            public_ips.append(ip)
    
    return public_ips
 
# 사설 IP 주소 여부를 확인하는 함수
def is_private_ip(ip):
    private_ip_ranges = [
("10.0.0.0", "10.255.255.255"),
("172.16.0.0", "172.31.255.255"),
("192.168.0.0", "192.168.255.255")
    ]
    
    for start, end in private_ip_ranges:
        if ip_address(start) <= ip_address(ip) <= ip_address(end):
            return True
    return False
 
#Create a helper function that will convert the markdwon into nicely formatted text
def to_markdown(text):
  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))
 
if __name__=="__main__":
    
    dataframe = parse_folder_to_dataframe(folder_path)    
    dic_df = dataframe.to_dict(orient='records')
    
    # 이상 탐지한 웹로그 출력
    filtered_logs = filter_logs_within_1_minute(dic_df)
    
    ### 분석 결과 출력 단계 ###
    # gemini-agent 구성
    #api_key = 'AIzaSyAij_2eLy-nzQVRDouKKur-1TfObHYi3E8' 
    genai.configure(api_key = gemini_api_key) #transport='' The user can pass a string to choose 'rest' or 'grpc' or 'grpc_asyncio'
    model = genai.GenerativeModel("models/gemini-1.5-pro-latest")
    ### 기본 출력 ###
    atk_scs, abnormal = attacked_web_log(filtered_logs)
    ### 취약점 스캔 기간 동안의 페이로드 분석 결과 출력 ###
    st.write("The results of the payload analysis during the hacker's vulnerability scan are as follows.")
    lst = atk_scs['request'].tolist()
    response = model.generate_content(f"{lst}: Please analyze those payloads")
    st.write(response)
    ### 취약점 스캔 이후 페이로드 분석 결과 출력 ###
    st.write("The following is the result of the payload analysis after the hacker's vulnerability scan.")
    pay_lst = abnormal['request'].tolist()
    response = model.generate_content(f"{pay_lst}: Please analyze the payloads. If there's nothing special, let me know there isn't")
    st.write(response)
    st.write("In addition, this is the result of an analysis of API security vulnerabilities after the vulnerability scan.")
    response = model.generate_content(f"{pay_lst}: Please let me know if there are any vulnerabilities that correspond to OWASP API security TOP10. If there is nothing special, please let me know there are none")
    st.write(response)



